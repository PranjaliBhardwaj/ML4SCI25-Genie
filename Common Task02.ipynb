{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11127179,"sourceType":"datasetVersion","datasetId":6939437}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task 02  Jets as graphs \nPlease choose a graph-based GNN model of your choice to classify (quark/gluon) jets. Proceed as follows:\nConvert the images into a point cloud dataset by only considering the non-zero pixels for every event.\nCast the point cloud data into a graph representation by coming up with suitable representations for nodes and edges.\nTrain your model on the obtained graph representations of the jet events.\nDiscuss the resulting performance of the chosen architecture. \n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GATConv, global_mean_pool, BatchNorm\nimport numpy as np\nimport h5py\nfrom sklearn.neighbors import NearestNeighbors\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with h5py.File(file_path, 'r') as f:\n    print(list(f.keys()))  # Print available keys in the HDF5 file\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:37:48.150998Z","iopub.execute_input":"2025-03-27T15:37:48.151334Z","iopub.status.idle":"2025-03-27T15:37:48.160165Z","shell.execute_reply.started":"2025-03-27T15:37:48.151305Z","shell.execute_reply":"2025-03-27T15:37:48.159461Z"}},"outputs":[{"name":"stdout","text":"['X_jets', 'm0', 'pt', 'y']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Define GAT-based GNN\nclass GATNet(nn.Module):\n    def __init__(self, input_dim=5, hidden_dim=128, output_dim=2, heads=4):\n        super(GATNet, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.1)\n        self.bn1 = BatchNorm(hidden_dim * heads)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=0.1)\n        self.bn2 = BatchNorm(hidden_dim * heads)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=0.1)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n        self.softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        # Layer 1\n        x = self.conv1(x, edge_index, edge_attr)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        # Layer 2 with residual-like connection\n        residual = x\n        x = self.conv2(x, edge_index, edge_attr)\n        x = self.bn2(x)\n        if x.shape == residual.shape:\n            x = x + residual  # Residual connection\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        # Layer 3\n        x = self.conv3(x, edge_index, edge_attr)\n        x = self.bn3(x)\n        x = self.relu(x)\n        \n        # Pooling and classification\n        x = global_mean_pool(x, batch)\n        x = self.fc(x)\n        return self.softmax(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model initialization\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = GATNet(input_dim=5, hidden_dim=128, output_dim=2, heads=4).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)  \noptimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = criterion(out, data.y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n        \n        total_loss += loss.item()\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n        total += data.y.size(0)\n    \n    scheduler.step()\n    avg_loss = total_loss / len(train_loader)\n    accuracy = correct / total\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n        total += data.y.size(0)\n\ntest_accuracy = correct / total\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T08:23:48.865922Z","iopub.execute_input":"2025-03-28T08:23:48.866317Z","iopub.status.idle":"2025-03-28T12:07:38.931772Z","shell.execute_reply.started":"2025-03-28T08:23:48.866288Z","shell.execute_reply":"2025-03-28T12:07:38.930878Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50, Loss: 0.6083, Accuracy: 0.6943\nEpoch 2/50, Loss: 0.6002, Accuracy: 0.7048\nEpoch 3/50, Loss: 0.5982, Accuracy: 0.7069\nEpoch 4/50, Loss: 0.5969, Accuracy: 0.7084\nEpoch 5/50, Loss: 0.5966, Accuracy: 0.7100\nEpoch 6/50, Loss: 0.5956, Accuracy: 0.7100\nEpoch 7/50, Loss: 0.5949, Accuracy: 0.7108\nEpoch 8/50, Loss: 0.5942, Accuracy: 0.7109\nEpoch 9/50, Loss: 0.5942, Accuracy: 0.7118\nEpoch 10/50, Loss: 0.5933, Accuracy: 0.7132\nEpoch 11/50, Loss: 0.5936, Accuracy: 0.7121\nEpoch 12/50, Loss: 0.5926, Accuracy: 0.7141\nEpoch 13/50, Loss: 0.5923, Accuracy: 0.7146\nEpoch 14/50, Loss: 0.5923, Accuracy: 0.7135\nEpoch 15/50, Loss: 0.5919, Accuracy: 0.7139\nEpoch 16/50, Loss: 0.5914, Accuracy: 0.7154\nEpoch 17/50, Loss: 0.5911, Accuracy: 0.7143\nEpoch 18/50, Loss: 0.5909, Accuracy: 0.7152\nEpoch 19/50, Loss: 0.5907, Accuracy: 0.7154\nEpoch 20/50, Loss: 0.5910, Accuracy: 0.7139\nEpoch 21/50, Loss: 0.5897, Accuracy: 0.7170\nEpoch 22/50, Loss: 0.5900, Accuracy: 0.7159\nEpoch 23/50, Loss: 0.5899, Accuracy: 0.7165\nEpoch 24/50, Loss: 0.5890, Accuracy: 0.7171\nEpoch 25/50, Loss: 0.5892, Accuracy: 0.7176\nEpoch 26/50, Loss: 0.5892, Accuracy: 0.7178\nEpoch 27/50, Loss: 0.5886, Accuracy: 0.7174\nEpoch 28/50, Loss: 0.5883, Accuracy: 0.7184\nEpoch 29/50, Loss: 0.5882, Accuracy: 0.7178\nEpoch 30/50, Loss: 0.5883, Accuracy: 0.7184\nEpoch 31/50, Loss: 0.5875, Accuracy: 0.7191\nEpoch 32/50, Loss: 0.5876, Accuracy: 0.7195\nEpoch 33/50, Loss: 0.5874, Accuracy: 0.7188\nEpoch 34/50, Loss: 0.5876, Accuracy: 0.7187\nEpoch 35/50, Loss: 0.5865, Accuracy: 0.7193\nEpoch 36/50, Loss: 0.5864, Accuracy: 0.7191\nEpoch 37/50, Loss: 0.5866, Accuracy: 0.7192\nEpoch 38/50, Loss: 0.5865, Accuracy: 0.7193\nEpoch 39/50, Loss: 0.5863, Accuracy: 0.7211\nEpoch 40/50, Loss: 0.5860, Accuracy: 0.7200\nEpoch 41/50, Loss: 0.5854, Accuracy: 0.7215\nEpoch 42/50, Loss: 0.5858, Accuracy: 0.7205\nEpoch 43/50, Loss: 0.5854, Accuracy: 0.7204\nEpoch 44/50, Loss: 0.5854, Accuracy: 0.7198\nEpoch 45/50, Loss: 0.5855, Accuracy: 0.7208\nEpoch 46/50, Loss: 0.5850, Accuracy: 0.7218\nEpoch 47/50, Loss: 0.5849, Accuracy: 0.7220\nEpoch 48/50, Loss: 0.5852, Accuracy: 0.7211\nEpoch 49/50, Loss: 0.5847, Accuracy: 0.7215\nEpoch 50/50, Loss: 0.5849, Accuracy: 0.7214\nTest Accuracy: 0.7171\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Why this approach? \nGAT is a good choice for Jet Classfication bebcause of its good handling of complex relationship with attention mechanism, multi head attention, helps with gradient flow, gradient clipping, batch normalisation, etc. whereas there could be some areas of improvement for better accuracy like optimising using various pooling techniques, model acrhitecture, data augmentation,etc.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
